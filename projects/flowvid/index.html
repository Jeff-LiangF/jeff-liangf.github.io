<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis">
  <meta name="keywords" content="video-to-video synthesis, diffusion models, optical flow">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis</title>

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/ut_icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    table {
      font-family: Arial, sans-serif;
      border-collapse: collapse;
      width: 100%;
    }
    td, th {
      border: 1px solid #dddddd;
      text-align: left;
      padding: 8px;
    }
    th {
      background-color: #dddddd;
    }
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://jeff-liangf.github.io/">Feng Liang</a><sup>* 1</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/bichenwu">Bichen Wu</a><sup>† 2</sup>,</span>
            <span class="author-block">
              <a href="https://sites.google.com/view/jialiangwang/home">Jialiang Wang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://lichengunc.github.io/">Licheng Yu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://kunpengli1994.github.io/">Kunpeng Li</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://yinan-zhao.github.io/">Yinan Zhao</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://imisra.github.io/">Ishan Misra</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/peizhao-zhang-14846042/">Peizhao Zhang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/site/vajdap">Peter Vajda</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.ece.utexas.edu/people/faculty/diana-marculescu">Diana Marculescu</a><sup>1</sup>
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The University of Texas at Austin,</span>
            <span class="author-block"><sup>2</sup>Meta GenAI</span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>*</sup>Work partially done during an internship at Meta GenAI.</span>
            <span class="author-block"><sup>†</sup>Corresponding author.</span>
          </div>
          <div class="is-size-4 publication-conference">
            <span class="conference-block">CVPR 2024 (Highlight)</span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2312.17681"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=xIUSG0pLNyo&t=119s"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
               <!--Code Link. -->
               <span class="link-block">
                <a href="supp/supp.html" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>Supplementary videos</span>
              </a>
            </span>
              <!--<span class="link-block">-->
                <!--<a href="https://github.com/Jeff-LiangF/FlowVid"-->
                   <!--class="external-link button is-normal is-rounded is-dark">-->
                  <!--<span class="icon">-->
                      <!--<i class="fab fa-github"></i>-->
                  <!--</span>-->
                  <!--<span>Code (coming)</span>-->
                  <!--</a>-->
              <!--</span>-->
               <!-- Demo Link. -->
              <!-- <span class="link-block">
                <a href="" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <svg class="svg-inline--fa fa-face-smiling-hands" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="face-smiling-hands" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" data-fa-i2svg=""><path fill="currentColor" d="M411.1 495.3C382.8 506.1 352.1 512 319.1 512C287.9 512 257.2 506.1 228.9 495.3C245.9 473.7 255.1 446.4 255.1 416.8V386C274.1 394.4 295.4 400 319.1 400C344.6 400 365.9 394.4 384 386V416.8C384 446.4 394.1 473.7 411.1 495.3V495.3zM575.7 242.6C558.8 236.8 539.5 240.6 526.1 254.1L478.1 301.1C469.6 287.4 453.9 278.4 436 278.4C407.3 278.4 384 301.7 384 330.4V349.6C367.2 360.3 345.9 368 319.1 368C294.1 368 272.8 360.3 255.1 349.6V330.4C255.1 301.7 232.7 278.4 203.1 278.4C186.1 278.4 170.4 287.4 161 301.1L113.9 254.1C100.5 240.6 81.15 236.8 64.34 242.6C71.31 107.5 183.1 0 319.1 0C456.9 0 568.7 107.5 575.7 242.6V242.6zM281.6 228.8C283.7 231.6 287.3 232.7 290.5 231.6C293.8 230.5 295.1 227.4 295.1 224C295.1 206.1 289.3 188.4 279.4 175.2C269.6 162.2 255.5 152 239.1 152C224.5 152 210.4 162.2 200.6 175.2C190.7 188.4 183.1 206.1 183.1 224C183.1 227.4 186.2 230.5 189.5 231.6C192.7 232.7 196.3 231.6 198.4 228.8L198.4 228.8L198.6 228.5C198.8 228.3 198.1 228 199.3 227.6C199.1 226.8 200.9 225.7 202.1 224.3C204.6 221.4 208.1 217.7 212.3 213.1C221.1 206.2 231.2 200 239.1 200C248.8 200 258.9 206.2 267.7 213.1C271.9 217.7 275.4 221.4 277.9 224.3C279.1 225.7 280 226.8 280.7 227.6C281 228 281.2 228.3 281.4 228.5L281.6 228.8L281.6 228.8zM450.5 231.6C453.8 230.5 456 227.4 456 224C456 206.1 449.3 188.4 439.4 175.2C429.6 162.2 415.5 152 400 152C384.5 152 370.4 162.2 360.6 175.2C350.7 188.4 344 206.1 344 224C344 227.4 346.2 230.5 349.5 231.6C352.7 232.7 356.3 231.6 358.4 228.8L358.4 228.8L358.6 228.5C358.8 228.3 358.1 228 359.3 227.6C359.1 226.8 360.9 225.7 362.1 224.3C364.6 221.4 368.1 217.7 372.3 213.1C381.1 206.2 391.2 200 400 200C408.8 200 418.9 206.2 427.7 213.1C431.9 217.7 435.4 221.4 437.9 224.3C439.1 225.7 440 226.8 440.7 227.6C441 228 441.2 228.3 441.4 228.5L441.6 228.8L441.6 228.8C443.7 231.6 447.3 232.7 450.5 231.6V231.6zM68.69 299.3C62.44 293.1 62.44 282.9 68.69 276.7C74.93 270.4 85.06 270.4 91.31 276.7L170.3 355.7C175.4 360.8 184 357.2 184 350.1V330.4C184 319.4 192.1 310.4 204 310.4C215 310.4 224 319.4 224 330.4V416.8C224 469.4 181.4 512 128.8 512C103.6 512 79.34 501.1 61.49 484.1L4.686 427.3C-1.562 421.1-1.562 410.9 4.686 404.7C10.93 398.4 21.07 398.4 27.31 404.7L46.63 424C49.22 426.6 53.41 426.6 55.1 424C58.59 421.4 58.59 417.2 55.1 414.6L4.686 363.3C-1.562 357.1-1.562 346.9 4.686 340.7C10.93 334.4 21.07 334.4 27.31 340.7L78.63 392C81.22 394.6 85.41 394.6 87.1 392C90.59 389.4 90.59 385.2 87.1 382.6L20.69 315.3C14.44 309.1 14.44 298.9 20.69 292.7C26.93 286.4 37.06 286.4 43.31 292.7L110.6 360C113.2 362.6 117.4 362.6 119.1 360C122.6 357.4 122.6 353.2 119.1 350.6L68.69 299.3zM520 350.6C517.4 353.2 517.4 357.4 520 360C522.6 362.6 526.8 362.6 529.4 360L596.7 292.7C602.9 286.4 613.1 286.4 619.3 292.7C625.6 298.9 625.6 309.1 619.3 315.3L552 382.6C549.4 385.2 549.4 389.4 552 392C554.6 394.6 558.8 394.6 561.4 392L612.7 340.7C618.9 334.4 629.1 334.4 635.3 340.7C641.6 346.9 641.6 357.1 635.3 363.3L584 414.6C581.4 417.2 581.4 421.4 584 424C586.6 426.6 590.8 426.6 593.4 424L612.7 404.7C618.9 398.4 629.1 398.4 635.3 404.7C641.6 410.9 641.6 421.1 635.3 427.3L578.5 484.1C560.7 501.1 536.4 512 511.2 512C458.6 512 416 469.4 416 416.8V330.4C416 319.4 424.1 310.4 436 310.4C447 310.4 456 319.4 456 330.4V350.1C456 357.2 464.6 360.8 469.7 355.7L548.7 276.7C554.9 270.4 565.1 270.4 571.3 276.7C577.6 282.9 577.6 293.1 571.3 299.3L520 350.6z"></path></svg>
                  </span>
                  <span>Demo (coming)</span>
                  </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="./static/images/flowvid.mp4" type="video/mp4">
      </video>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- 
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="demo" autoplay muted loop playsinline height="100%">
        <source src="./static/images/ovseg.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        We present FlowVid to synthesize a consistent video given an input video and a target prompt.
      </h2>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Diffusion models have transformed the image-to-image (I2I) synthesis and are now permeating into videos. 
            However, the advancement of video-to-video (V2V) synthesis has been hampered by the challenge of maintaining temporal consistency across video frames. 
            This paper proposes a consistent V2V synthesis framework by jointly leveraging spatial conditions and temporal optical flow clues within thesource video. 
            Contrary to prior methods that strictly adhere to optical flow, our approach harnesses its benefits while handling the imperfection in flow estimation. 
            We encode the optical flow via warping from the first frame and serve it as a supplementary reference in the diffusion model.
            This enables our model for video synthesis by editing the first frame with any prevalent I2I models, and then propagating edits to successive frames. 
            Our V2V model, FlowVid, demonstrates remarkable properties: 
            (1) <span style="color: orange; font-weight:bold">Flexibility</span>: FlowVid works seamlessly with existing I2I models, facilitating various modifications, including stylization, object swaps, and local edits.
            (2) <span style="color: orange; font-weight:bold">Efficiency</span>: Generation of a 4-second video with 30 FPS and 512×512 resolution takes only 1.5 minutes, which is 3.1×, 7.2×, and 10.5× faster than CoDeF, Rerender, and TokenFlow, respectively.
            (3) <span style="color: orange; font-weight:bold">High-quality</span>: In user studies, our FlowVid is preferred 45.7% of the time, outperforming CoDeF (3.5%), Rerender (10.2%), and TokenFlow (40.4%).
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!--&lt;!&ndash; Paper video. &ndash;&gt;-->
    <!--<div class="columns is-centered has-text-centered">-->
      <!--<div class="column is-four-fifths">-->
        <!--<h2 class="title is-3">Video</h2>-->
        <!--<div class="publication-video">-->
          <!--<iframe src="h"-->
                  <!--frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
        <!--</div>-->
      <!--</div>-->
    <!--</div>-->
    <!--&lt;!&ndash;/ Paper video. &ndash;&gt;-->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">


    <div class="columns is-centered">
      <div class="column is-full-width">

        <!-- Motivation. -->
        <h2 class="title is-3">Motivation</h2>

        <div class="subtitle has-text-justified">
          <p>
          Optical flow is widely used in video-to-video models, However, <span style="color: orange; font-weight:bold">estimated flow can be inaccurate</span>.
          We propose to use spatial controls in synergy to rectify the inaccurate flow to synthesize consistent output.
          </p>
        <div class="container is-max-desktop">
          <div class="hero-body">
            <video id="motivation" autoplay muted loop playsinline width="640" height="360">
              <source src="./static/images/motivation.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        <!--/ Motivation. -->

        <!-- Method. -->
        <h2 class="title is-3">Method</h2>

        <div class="subtitle has-text-justified">
          <p> </p>
          <p>
          We train a video diffusion model with joint spatial-temporal controls.
          During generation, we edit the first frame with existing I2I models, then feed the spatial controls and warped video to our trained model.
          </p>
        </div>

        <div class="container is-max-desktop">
          <div class="hero-body">
            <video id="method" autoplay muted loop playsinline height="100%">
              <source src="./static/images/method.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        <!--/ Method. -->

        <!-- Results. -->
        <h2 class="title is-3">FlowVid Highlights</h2>

        <div class="subtitle has-text-justified">
          <p>
            <span style="color: orange; font-weight:bold">Use your cursor on the video to slide left/right.</span>. The left is the input video, the right is the synthesized video. The edited keyword is marked as <span style="color: rgb(0, 165, 255)">blue.</span>
            You may also want to check 
            <a href="https://jeff-liangf.github.io/projects/flowvid/supp/supp.html#comparisons_baselines_container">comparsion with other methods</a>,
            <a href="https://jeff-liangf.github.io/projects/flowvid/supp/supp.html#Ablations">ablations</a>, and
            <a href="https://jeff-liangf.github.io/projects/flowvid/supp/supp.html#Limitations">limitations</a> in the 
            <a href="https://jeff-liangf.github.io/projects/flowvid/supp/supp.html">supplimentart videos</a>
          </p>
        </div>    

          <div>
              <video class="video" id="Teaser1" width="100%" loop playsinline autoPlay muted src="./supp/new_assets/stylization.mp4" onplay="resizeAndPlay(this)"></video>
              <canvas width="150" height="150" class="videoMerge" id="Teaser1Merge"></canvas>
          </div>
          <p style="text-align: center;">
            <span style="color: orange; font-weight:bold">Stylization Highlights.</span>
          </p>
          <br>
        
          <div>
            <video class="video" id="Teaser2"  width="100%"  loop playsinline autoPlay muted src="./supp/new_assets/object_swap.mp4" onplay="resizeAndPlay(this)"></video>
            <canvas width="150" height="150" class="videoMerge" id="Teaser2Merge"></canvas>
          </div>
          <p style="text-align: center;">
            <span style="color: orange; font-weight:bold">ObjectSwap Highlights.</span>
          </p>
          <br>
          
        <h2 class="title is-3">Quantitative Comparsion with other methods</h2>

        <div class="subtitle has-text-justified">
          <p>
            We conduct a user study on 25 DAVIS videos and 115 manually designed prompts.
            For results on DAVIS, please see <a href="https://drive.google.com/drive/u/0/folders/1sxwhorvWI6EgM8qBmU3thUGeunEnESxQ">this link</a>.
          </p>
        </div>    

          <table>
            <tr>
              <th></th>
              <th>Preference rate (mean ± std %) ↑</th>
              <th>Runtime (mins) ↓</th>
              <th>Cost ↓</th>
            </tr>
            <tr>
              <td>TokenFlow</td>
              <td>40.4 ± 5.3</td>
              <td>15.8</td>
              <td>10.5 ×</td>
            </tr>
            <tr>
              <td>Rerender</td>
              <td>10.2 ± 7.1</td>
              <td>10.8</td>
              <td>7.2 ×</td>
            </tr>
            <tr>
              <td>CoDeF</td>
              <td>3.5 ± 1.9</td>
              <td>4.6</td>
              <td>3.1 ×</td>
            </tr>
            <tr>
              <td>FlowVid (Ours)</td>
              <td><span style="color: orange; font-weight:bold">45.7</span> ± 6.4</td>
              <td><span style="color: orange; font-weight:bold">1.5</span></td>
              <td><span style="color: orange; font-weight:bold">1.0 ×</span></td>
            </tr>
          </table>
          <p>
          <span style="color: orange; font-weight:bold">Quantitative comparison with existing V2V models.</span> The preference rate indicates the frequency the method is preferred among all the four methods in human evaluation. Runtime shows the time to synthesize a 4-second video with 512x512 resolution on one A-100-80GB. Cost is normalized with our method.</p>


      </div>
    </div>


  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
        <pre><code>@article{liang2023flowvid,
  title={FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis},
  author={Liang, Feng and Wu, Bichen and Wang, Jialiang and Yu, Licheng and Li, Kunpeng and Zhao, Yinan and Misra, Ishan and Huang, Jia-Bin and Zhang, Peizhao and Vajda, Peter and others},
  journal={arXiv preprint arXiv:2312.17681},
  year={2023}
}
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2312.17681.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/Jeff-LiangF/FlowVid" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Thanks to <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
