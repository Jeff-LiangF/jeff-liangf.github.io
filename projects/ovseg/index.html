<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP">
  <meta name="keywords" content="Vision-language models, Open-vocabulary segmentation, CLIP">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP</title>

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/ut_icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://jeff-liangf.github.io/">Feng Liang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/bichenwu">Bichen Wu</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://sites.google.com/view/xiaoliangdai/">Xiaoliang Dai</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://kunpengli1994.github.io/">Kunpeng Li</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://yinan-zhao.github.io/">Yinan Zhao</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://hangzhang.org/">Hang Zhang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/peizhao-zhang-14846042/">Peizhao Zhang</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/site/vajdap">Peter Vajda</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.ece.utexas.edu/people/faculty/diana-marculescu">Diana Marculescu</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The University of Texas at Austin,</span>
            <span class="author-block"><sup>2</sup>Meta Reality Labs,</span>
            <span class="author-block"><sup>3</sup>Cruise</span>
          </div>

          <div class="is-size-4 publication-conference">
            <span class="conference-block">CVPR 2023</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2210.04150"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=xIUSG0pLNyo&t=119s"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
               <!--Code Link. -->
              <span class="link-block">
                <a href="https://github.com/facebookresearch/ov-seg"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/spaces/facebook/ov-seg" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <svg class="svg-inline--fa fa-face-smiling-hands" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="face-smiling-hands" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" data-fa-i2svg=""><path fill="currentColor" d="M411.1 495.3C382.8 506.1 352.1 512 319.1 512C287.9 512 257.2 506.1 228.9 495.3C245.9 473.7 255.1 446.4 255.1 416.8V386C274.1 394.4 295.4 400 319.1 400C344.6 400 365.9 394.4 384 386V416.8C384 446.4 394.1 473.7 411.1 495.3V495.3zM575.7 242.6C558.8 236.8 539.5 240.6 526.1 254.1L478.1 301.1C469.6 287.4 453.9 278.4 436 278.4C407.3 278.4 384 301.7 384 330.4V349.6C367.2 360.3 345.9 368 319.1 368C294.1 368 272.8 360.3 255.1 349.6V330.4C255.1 301.7 232.7 278.4 203.1 278.4C186.1 278.4 170.4 287.4 161 301.1L113.9 254.1C100.5 240.6 81.15 236.8 64.34 242.6C71.31 107.5 183.1 0 319.1 0C456.9 0 568.7 107.5 575.7 242.6V242.6zM281.6 228.8C283.7 231.6 287.3 232.7 290.5 231.6C293.8 230.5 295.1 227.4 295.1 224C295.1 206.1 289.3 188.4 279.4 175.2C269.6 162.2 255.5 152 239.1 152C224.5 152 210.4 162.2 200.6 175.2C190.7 188.4 183.1 206.1 183.1 224C183.1 227.4 186.2 230.5 189.5 231.6C192.7 232.7 196.3 231.6 198.4 228.8L198.4 228.8L198.6 228.5C198.8 228.3 198.1 228 199.3 227.6C199.1 226.8 200.9 225.7 202.1 224.3C204.6 221.4 208.1 217.7 212.3 213.1C221.1 206.2 231.2 200 239.1 200C248.8 200 258.9 206.2 267.7 213.1C271.9 217.7 275.4 221.4 277.9 224.3C279.1 225.7 280 226.8 280.7 227.6C281 228 281.2 228.3 281.4 228.5L281.6 228.8L281.6 228.8zM450.5 231.6C453.8 230.5 456 227.4 456 224C456 206.1 449.3 188.4 439.4 175.2C429.6 162.2 415.5 152 400 152C384.5 152 370.4 162.2 360.6 175.2C350.7 188.4 344 206.1 344 224C344 227.4 346.2 230.5 349.5 231.6C352.7 232.7 356.3 231.6 358.4 228.8L358.4 228.8L358.6 228.5C358.8 228.3 358.1 228 359.3 227.6C359.1 226.8 360.9 225.7 362.1 224.3C364.6 221.4 368.1 217.7 372.3 213.1C381.1 206.2 391.2 200 400 200C408.8 200 418.9 206.2 427.7 213.1C431.9 217.7 435.4 221.4 437.9 224.3C439.1 225.7 440 226.8 440.7 227.6C441 228 441.2 228.3 441.4 228.5L441.6 228.8L441.6 228.8C443.7 231.6 447.3 232.7 450.5 231.6V231.6zM68.69 299.3C62.44 293.1 62.44 282.9 68.69 276.7C74.93 270.4 85.06 270.4 91.31 276.7L170.3 355.7C175.4 360.8 184 357.2 184 350.1V330.4C184 319.4 192.1 310.4 204 310.4C215 310.4 224 319.4 224 330.4V416.8C224 469.4 181.4 512 128.8 512C103.6 512 79.34 501.1 61.49 484.1L4.686 427.3C-1.562 421.1-1.562 410.9 4.686 404.7C10.93 398.4 21.07 398.4 27.31 404.7L46.63 424C49.22 426.6 53.41 426.6 55.1 424C58.59 421.4 58.59 417.2 55.1 414.6L4.686 363.3C-1.562 357.1-1.562 346.9 4.686 340.7C10.93 334.4 21.07 334.4 27.31 340.7L78.63 392C81.22 394.6 85.41 394.6 87.1 392C90.59 389.4 90.59 385.2 87.1 382.6L20.69 315.3C14.44 309.1 14.44 298.9 20.69 292.7C26.93 286.4 37.06 286.4 43.31 292.7L110.6 360C113.2 362.6 117.4 362.6 119.1 360C122.6 357.4 122.6 353.2 119.1 350.6L68.69 299.3zM520 350.6C517.4 353.2 517.4 357.4 520 360C522.6 362.6 526.8 362.6 529.4 360L596.7 292.7C602.9 286.4 613.1 286.4 619.3 292.7C625.6 298.9 625.6 309.1 619.3 315.3L552 382.6C549.4 385.2 549.4 389.4 552 392C554.6 394.6 558.8 394.6 561.4 392L612.7 340.7C618.9 334.4 629.1 334.4 635.3 340.7C641.6 346.9 641.6 357.1 635.3 363.3L584 414.6C581.4 417.2 581.4 421.4 584 424C586.6 426.6 590.8 426.6 593.4 424L612.7 404.7C618.9 398.4 629.1 398.4 635.3 404.7C641.6 410.9 641.6 421.1 635.3 427.3L578.5 484.1C560.7 501.1 536.4 512 511.2 512C458.6 512 416 469.4 416 416.8V330.4C416 319.4 424.1 310.4 436 310.4C447 310.4 456 319.4 456 330.4V350.1C456 357.2 464.6 360.8 469.7 355.7L548.7 276.7C554.9 270.4 565.1 270.4 571.3 276.7C577.6 282.9 577.6 293.1 571.3 299.3L520 350.6z"></path></svg><!-- <i class="fa-solid fa-face-smiling-hands"></i> -->
                  </span>
                  <span>Demo</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="demo" autoplay muted loop playsinline height="100%">
        <source src="./static/images/ovseg.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Our model can perform open-vocabulary segmentation with <span style="color: orange; font-weight:bold">user-defined arbitrary queries</span>.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Open-vocabulary semantic segmentation aims to segment an image into semantic regions according to text descriptions,
            which may not have been seen during training.
          Recent two-stage methods first generate class-agnostic mask proposals and then leverage pre-trained vision-language models,
            <i>e.g.</i>, CLIP, to classify masked regions.
          We identify the performance bottleneck of this paradigm to be the pre-trained CLIP model,
            since it does not perform well on masked images.
          To address this, we propose to finetune CLIP on a collection of masked image regions and their corresponding text descriptions.
            We collect training data by mining an existing image-caption dataset (<i>e.g.</i>, COCO Captions),
            using CLIP to match masked image regions to nouns in the image captions.
          Compared with the more precise and manually annotated segmentation labels with fixed classes (<i>e.g.</i>, COCO-Stuff),
            we find our noisy but diverse dataset can better retain CLIP's generalization ability.
          Along with finetuning the entire model, we utilize the "blank" areas in masked images using a method we dub <span style="color: orange; font-weight:bold">mask prompt tuning</span>.
          Experiments demonstrate mask prompt tuning brings significant improvement without modifying any weights of CLIP,
            and it can further improve a fully finetuned model.
          In particular, when trained on COCO and evaluated on ADE20K-150, our best model achieves 29.6% mIoU,
            which is +8.5% higher than the previous state-of-the-art.
          <span style="color: orange; font-weight:bold">For the first time, open-vocabulary generalist models match the performance
            of supervised specialist models in 2017 without dataset specific adaptations</span>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!--&lt;!&ndash; Paper video. &ndash;&gt;-->
    <!--<div class="columns is-centered has-text-centered">-->
      <!--<div class="column is-four-fifths">-->
        <!--<h2 class="title is-3">Video</h2>-->
        <!--<div class="publication-video">-->
          <!--<iframe src="h"-->
                  <!--frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
        <!--</div>-->
      <!--</div>-->
    <!--</div>-->
    <!--&lt;!&ndash;/ Paper video. &ndash;&gt;-->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">


    <div class="columns is-centered">
      <div class="column is-full-width">

        <!-- Motivation. -->
        <h2 class="title is-3">Motivation</h2>

        <div class="subtitle has-text-justified">
          <p>
          Our analysis reveals the pre-trained CLIP does <span style="color: orange; font-weight:bold">not</span> perform well on mask proposals,
            making it the performance bottleneck of two-stage approaches.
          </p>
        </div>

        <div class="container is-max-desktop">
          <div class="hero-body">
            <video id="motivation" autoplay muted loop playsinline height="100%">
              <source src="./static/images/motivation.mp4"
                      type="video/mp4">
            </video>
            <h2 align="left">
              <p>(a) CLIP is pre-trained with natural images with little data augmentation.</p>
              <p>(b) Two-stage open-vocabulary semantic segmentation approaches first generate class-agnostic mask proposals
              and then leverage pre-trained CLIP to do open-vocabulary classification.
              The input of the CLIP model is cropped masked images, which have huge domain gap from the natural images.</p>
              <p>(c) Our analysis reveals that pre-trained CLIP does not work well on masked images.</p>
            </h2>
          </div>
        </div>


        <br/>
        <!--/ Motivation. -->

        <!-- Method. -->
        <h2 class="title is-3">Method</h2>

        <div class="subtitle has-text-justified">
          <p> </p>
          <p>
          Our model consists of one segmentation model, <i>e.g.</i>, MaskFormer, and one CLIP model.
          </p>
        </div>

        <div class="container is-max-desktop">
          <div class="hero-body">
            <video id="method" autoplay muted loop playsinline height="100%">
              <source src="./static/images/method.mp4"
                      type="video/mp4">
            </video>
            <h2 align="left">
              <p>We first train the modified MaskFormer as the open-vocabulary segmentation baseline (Section 3.1).
	      Then we collect diverse mask-category pairs from image captions (Section 3.2) and adapt CLIP for masked images  (Section 3.3).</p>
            </h2>
          </div>
        </div>
        <!--/ Method. -->

        <!-- Results. -->
        <h2 class="title is-3">Results</h2>

        <div class="subtitle has-text-justified">
          <p>
            For the first time, we show open-vocabulary <span style="color: orange; font-weight:bold">generalist</span>
            models can match the performance of supervised <span style="color: orange; font-weight:bold">specialist</span> models
              without dataset specific adaptations.
          </p>

        </div>

        <style type="text/css">
        .tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;margin:0px auto;}
        .tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;
          font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}
        .tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;
          font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
        .tg .tg-cey4{border-color:inherit;font-size:16px;text-align:left;vertical-align:top}
        .tg .tg-x5q1{font-size:16px;text-align:left;vertical-align:top}
        .tg .tg-o5pl{color:#f56b00;font-size:16px;font-weight:bold;text-align:left;vertical-align:top}
        .tg .tg-oame{font-size:16px;font-style:italic;text-align:center;vertical-align:top}
        .tg .tg-fa52{color:#656565;font-size:16px;text-align:center;vertical-align:top}
        .tg .tg-um6g{color:#656565;font-size:16px;text-align:left;vertical-align:top}
        </style>
        <table class="tg">
        <thead>
          <tr>
            <th class="tg-cey4">method</th>
            <th class="tg-cey4">backbone</th>
            <th class="tg-cey4">training dataset</th>
            <th class="tg-cey4">A-847</th>
            <th class="tg-x5q1">PC-459</th>
            <th class="tg-x5q1">A-150</th>
            <th class="tg-x5q1">PC-59</th>
            <th class="tg-x5q1">PAS-20</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="tg-oame" colspan="8"><span style="font-weight:400;font-style:italic">Open-vocabulary generalist models.</span></td>
          </tr>
          <tr>
            <td class="tg-cey4">SPNet</td>
            <td class="tg-cey4">R-101</td>
            <td class="tg-cey4">PASCAL-15</td>
            <td class="tg-cey4">-</td>
            <td class="tg-x5q1">-</td>
            <td class="tg-x5q1">-</td>
            <td class="tg-x5q1">24.3</td>
            <td class="tg-x5q1">18.3</td>
          </tr>
          <tr>
            <td class="tg-cey4">ZS3Net</td>
            <td class="tg-cey4">R-101</td>
            <td class="tg-cey4">PASCAL-15</td>
            <td class="tg-cey4">-</td>
            <td class="tg-x5q1">-</td>
            <td class="tg-x5q1">-</td>
            <td class="tg-x5q1">19.4</td>
            <td class="tg-x5q1">38.3</td>
          </tr>
          <tr>
            <td class="tg-x5q1">LSeg</td>
            <td class="tg-x5q1">R-101</td>
            <td class="tg-x5q1">PASCAL-15</td>
            <td class="tg-x5q1">-</td>
            <td class="tg-x5q1">-</td>
            <td class="tg-x5q1">-</td>
            <td class="tg-x5q1">-</td>
            <td class="tg-x5q1">47.4</td>
          </tr>
          <tr>
            <td class="tg-x5q1">LSeg+</td>
            <td class="tg-x5q1">R-101</td>
            <td class="tg-x5q1">COCO Panoptic</td>
            <td class="tg-x5q1">2.5</td>
            <td class="tg-x5q1">5.2</td>
            <td class="tg-x5q1">13.0</td>
            <td class="tg-x5q1">36.0</td>
            <td class="tg-x5q1">59.0</td>
          </tr>
          <tr>
            <td class="tg-x5q1">SimBaseline</td>
            <td class="tg-x5q1">R-101c</td>
            <td class="tg-x5q1">COCO-Stuff-156</td>
            <td class="tg-x5q1">-</td>
            <td class="tg-x5q1">-</td>
            <td class="tg-x5q1">15.3</td>
            <td class="tg-x5q1">-</td>
            <td class="tg-x5q1">74.5</td>
          </tr>
          <tr>
            <td class="tg-x5q1">ZegFormer</td>
            <td class="tg-x5q1">R-50</td>
            <td class="tg-x5q1">COCO-Stuff-156</td>
            <td class="tg-x5q1">-</td>
            <td class="tg-x5q1">-</td>
            <td class="tg-x5q1">16.4</td>
            <td class="tg-x5q1">-</td>
            <td class="tg-x5q1">80.7</td>
          </tr>
          <tr>
            <td class="tg-x5q1">OpenSeg</td>
            <td class="tg-x5q1">R-101</td>
            <td class="tg-x5q1">COCO Panoptic</td>
            <td class="tg-x5q1">4.0</td>
            <td class="tg-x5q1">6.5</td>
            <td class="tg-x5q1">15.3</td>
            <td class="tg-x5q1">36.9</td>
            <td class="tg-x5q1">60.0</td>
          </tr>
          <tr>
            <td class="tg-x5q1">OVSeg (Ours)</td>
            <td class="tg-x5q1">R-101c</td>
            <td class="tg-x5q1">COCO-Stuff-171</td>
            <td class="tg-o5pl">7.1</td>
            <td class="tg-o5pl">11.0</td>
            <td class="tg-o5pl">24.8</td>
            <td class="tg-o5pl">53.3</td>
            <td class="tg-o5pl">92.6</td>
          </tr>
          <tr>
            <td class="tg-x5q1">LSeg+</td>
            <td class="tg-x5q1">Eff-B7</td>
            <td class="tg-x5q1">COCO Panoptic</td>
            <td class="tg-x5q1">3.8</td>
            <td class="tg-x5q1">7.8</td>
            <td class="tg-x5q1">18.0</td>
            <td class="tg-x5q1">46.5</td>
            <td class="tg-x5q1">-</td>
          </tr>
          <tr>
            <td class="tg-x5q1">OpenSeg</td>
            <td class="tg-x5q1">Eff-B7</td>
            <td class="tg-x5q1">COCO Panoptic</td>
            <td class="tg-x5q1"><span style="font-weight:400;font-style:normal">6.3</span></td>
            <td class="tg-x5q1">9.0</td>
            <td class="tg-x5q1">21.1</td>
            <td class="tg-x5q1">42.1</td>
            <td class="tg-x5q1">-</td>
          </tr>
          <tr>
            <td class="tg-x5q1">OVSeg (Ours)</td>
            <td class="tg-x5q1">Swin-B</td>
            <td class="tg-x5q1">COCO-Stuff-171</td>
            <td class="tg-o5pl">9.0</td>
            <td class="tg-o5pl">12.4</td>
            <td class="tg-o5pl">29.6</td>
            <td class="tg-o5pl">55.7</td>
            <td class="tg-o5pl">94.5</td>
          </tr>
          <tr>
            <td class="tg-fa52" colspan="8"><span style="font-weight:400;font-style:italic">Supervised specialist models.</span></td>
          </tr>
          <tr>
            <td class="tg-um6g">FCN</td>
            <td class="tg-um6g">FCN-8s</td>
            <td class="tg-um6g">Same as test</td>
            <td class="tg-um6g">-</td>
            <td class="tg-um6g">-</td>
            <td class="tg-um6g">29.4</td>
            <td class="tg-um6g">37.8</td>
            <td class="tg-um6g">-</td>
          </tr>
          <tr>
            <td class="tg-um6g">Deeplab</td>
            <td class="tg-um6g">R-101</td>
            <td class="tg-um6g">Same as test</td>
            <td class="tg-um6g">-</td>
            <td class="tg-um6g">-</td>
            <td class="tg-um6g">-</td>
            <td class="tg-um6g">45.7</td>
            <td class="tg-um6g">77.7</td>
          </tr>
          <tr>
            <td class="tg-um6g">SelfTrain</td>
            <td class="tg-um6g">Eff-L2</td>
            <td class="tg-um6g">Same as test</td>
            <td class="tg-um6g">-</td>
            <td class="tg-um6g">-</td>
            <td class="tg-um6g">-</td>
            <td class="tg-um6g">-</td>
            <td class="tg-um6g">90.0</td>
          </tr>
        </tbody>
        </table>

        <br/>
        <!--/ Motivation. -->


      </div>
    </div>


  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
        <pre><code>@inproceedings{liang2023open,
  title={Open-vocabulary semantic segmentation with mask-adapted clip},
  author={Liang, Feng and Wu, Bichen and Dai, Xiaoliang and Li, Kunpeng and Zhao, Yinan and Zhang, Hang and Zhang, Peizhao and Vajda, Peter and Marculescu, Diana},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7061--7070},
  year={2023}
}
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2210.04150.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/facebookresearch/ov-seg" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Thanks to <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a href="https://jerryxu.net/GroupViT">GroupViT</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
