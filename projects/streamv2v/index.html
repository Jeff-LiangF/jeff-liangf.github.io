<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Looking Backward: Streaming Video-to-Video Translation with Feature Banks">
  <meta name="keywords" content="video-to-video synthesis, diffusion models, video streaming">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Looking Backward: Streaming Video-to-Video Translation with Feature Banks</title>

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/ut_icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    table {
      font-family: Arial, sans-serif;
      border-collapse: collapse;
      width: 100%;
    }
    td, th {
      border: 1px solid #dddddd;
      text-align: left;
      padding: 8px;
    }
    th {
      background-color: #dddddd;
    }
    p {
        line-height: 1.6; /* Adjust the line height as needed */
    }
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Looking Backward: Streaming Video-to-Video Translation with Feature Banks</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://jeff-liangf.github.io/">Feng Liang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.co.jp/citations?user=15X3cioAAAAJ&hl=ja">Akio Kodaira</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.chenfengx.com/">Chenfeng Xu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://me.berkeley.edu/people/masayoshi-tomizuka/">Masayoshi Tomizuka</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~keutzer/">Kurt Keutzer</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.ece.utexas.edu/people/faculty/diana-marculescu">Diana Marculescu</a><sup>1</sup>
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The University of Texas at Austin,</span>
            <span class="author-block"><sup>2</sup>University of California, Berkeley</span>
          </div>
          <div class="is-size-4 publication-conference">
            <span class="conference-block">ICLR 2025</span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2405.15757"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=xIUSG0pLNyo&t=119s"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
               <!--Code Link. -->
               <span class="link-block">
                <a href="supp/supp.html" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>Supplementary videos</span>
              </a>
            </span>
              <span class="link-block">
                <a href="https://github.com/Jeff-LiangF/streamv2v"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
               <!-- Demo Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/spaces/JeffLiang/streamv2v" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <svg class="svg-inline--fa fa-face-smiling-hands" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="face-smiling-hands" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" data-fa-i2svg=""><path fill="currentColor" d="M411.1 495.3C382.8 506.1 352.1 512 319.1 512C287.9 512 257.2 506.1 228.9 495.3C245.9 473.7 255.1 446.4 255.1 416.8V386C274.1 394.4 295.4 400 319.1 400C344.6 400 365.9 394.4 384 386V416.8C384 446.4 394.1 473.7 411.1 495.3V495.3zM575.7 242.6C558.8 236.8 539.5 240.6 526.1 254.1L478.1 301.1C469.6 287.4 453.9 278.4 436 278.4C407.3 278.4 384 301.7 384 330.4V349.6C367.2 360.3 345.9 368 319.1 368C294.1 368 272.8 360.3 255.1 349.6V330.4C255.1 301.7 232.7 278.4 203.1 278.4C186.1 278.4 170.4 287.4 161 301.1L113.9 254.1C100.5 240.6 81.15 236.8 64.34 242.6C71.31 107.5 183.1 0 319.1 0C456.9 0 568.7 107.5 575.7 242.6V242.6zM281.6 228.8C283.7 231.6 287.3 232.7 290.5 231.6C293.8 230.5 295.1 227.4 295.1 224C295.1 206.1 289.3 188.4 279.4 175.2C269.6 162.2 255.5 152 239.1 152C224.5 152 210.4 162.2 200.6 175.2C190.7 188.4 183.1 206.1 183.1 224C183.1 227.4 186.2 230.5 189.5 231.6C192.7 232.7 196.3 231.6 198.4 228.8L198.4 228.8L198.6 228.5C198.8 228.3 198.1 228 199.3 227.6C199.1 226.8 200.9 225.7 202.1 224.3C204.6 221.4 208.1 217.7 212.3 213.1C221.1 206.2 231.2 200 239.1 200C248.8 200 258.9 206.2 267.7 213.1C271.9 217.7 275.4 221.4 277.9 224.3C279.1 225.7 280 226.8 280.7 227.6C281 228 281.2 228.3 281.4 228.5L281.6 228.8L281.6 228.8zM450.5 231.6C453.8 230.5 456 227.4 456 224C456 206.1 449.3 188.4 439.4 175.2C429.6 162.2 415.5 152 400 152C384.5 152 370.4 162.2 360.6 175.2C350.7 188.4 344 206.1 344 224C344 227.4 346.2 230.5 349.5 231.6C352.7 232.7 356.3 231.6 358.4 228.8L358.4 228.8L358.6 228.5C358.8 228.3 358.1 228 359.3 227.6C359.1 226.8 360.9 225.7 362.1 224.3C364.6 221.4 368.1 217.7 372.3 213.1C381.1 206.2 391.2 200 400 200C408.8 200 418.9 206.2 427.7 213.1C431.9 217.7 435.4 221.4 437.9 224.3C439.1 225.7 440 226.8 440.7 227.6C441 228 441.2 228.3 441.4 228.5L441.6 228.8L441.6 228.8C443.7 231.6 447.3 232.7 450.5 231.6V231.6zM68.69 299.3C62.44 293.1 62.44 282.9 68.69 276.7C74.93 270.4 85.06 270.4 91.31 276.7L170.3 355.7C175.4 360.8 184 357.2 184 350.1V330.4C184 319.4 192.1 310.4 204 310.4C215 310.4 224 319.4 224 330.4V416.8C224 469.4 181.4 512 128.8 512C103.6 512 79.34 501.1 61.49 484.1L4.686 427.3C-1.562 421.1-1.562 410.9 4.686 404.7C10.93 398.4 21.07 398.4 27.31 404.7L46.63 424C49.22 426.6 53.41 426.6 55.1 424C58.59 421.4 58.59 417.2 55.1 414.6L4.686 363.3C-1.562 357.1-1.562 346.9 4.686 340.7C10.93 334.4 21.07 334.4 27.31 340.7L78.63 392C81.22 394.6 85.41 394.6 87.1 392C90.59 389.4 90.59 385.2 87.1 382.6L20.69 315.3C14.44 309.1 14.44 298.9 20.69 292.7C26.93 286.4 37.06 286.4 43.31 292.7L110.6 360C113.2 362.6 117.4 362.6 119.1 360C122.6 357.4 122.6 353.2 119.1 350.6L68.69 299.3zM520 350.6C517.4 353.2 517.4 357.4 520 360C522.6 362.6 526.8 362.6 529.4 360L596.7 292.7C602.9 286.4 613.1 286.4 619.3 292.7C625.6 298.9 625.6 309.1 619.3 315.3L552 382.6C549.4 385.2 549.4 389.4 552 392C554.6 394.6 558.8 394.6 561.4 392L612.7 340.7C618.9 334.4 629.1 334.4 635.3 340.7C641.6 346.9 641.6 357.1 635.3 363.3L584 414.6C581.4 417.2 581.4 421.4 584 424C586.6 426.6 590.8 426.6 593.4 424L612.7 404.7C618.9 398.4 629.1 398.4 635.3 404.7C641.6 410.9 641.6 421.1 635.3 427.3L578.5 484.1C560.7 501.1 536.4 512 511.2 512C458.6 512 416 469.4 416 416.8V330.4C416 319.4 424.1 310.4 436 310.4C447 310.4 456 319.4 456 330.4V350.1C456 357.2 464.6 360.8 469.7 355.7L548.7 276.7C554.9 270.4 565.1 270.4 571.3 276.7C577.6 282.9 577.6 293.1 571.3 299.3L520 350.6z"></path></svg>
                  </span>
                  <span>Demo</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title" style="text-align: center;">Real-time V2V demo on one RTX 4090 GPU</h2>
      <video poster="" id="tree" controls loop height="100%">
        <source src="./static/images/camera_demo.mp4" type="video/mp4">
      </video>
    </div>
  </div>
</section>
<!-- End teaser video -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p style="font-size: 1.25em;">
            This paper introduces StreamV2V, a diffusion model that achieves real-time streaming video-to-video (V2V) translation with user prompts. 
            Unlike prior V2V methods using batches to process limited frames, we opt to process frames in a streaming fashion, to support unlimited frames.
            At the heart of StreamV2V lies a backward-looking principle that relates the present to the past. 
            This is realized by maintaining a feature bank, which archives information from past frames.
            For incoming frames, StreamV2V extends self-attention to include banked keys and values and directly fuses similar past features into the output.
            The feature bank is continually updated by merging stored and new features, making it compact but informative.
            StreamV2V stands out for its adaptability and efficiency, seamlessly integrating with image diffusion models without fine-tuning.
            It can run 20 FPS on one A100 GPU, being 15×, 46×, 108×, and 158× faster than FlowVid, CoDeF, Rerender, and TokenFlow, respectively. 
            Quantitative metrics and user studies confirm StreamV2V's exceptional ability to maintain temporal consistency.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">

        <!-- Highlight. -->
        <h2 class="title is-3">Performance Highlight</h2>

        <div class="subtitle has-text-justified">
          <p >
            We present StreamV2V to support real-time video-to-video translation for streaming input. 
            For webcam input, our StreamV2V supports face swap (<i>e.g.,</i> to <code>Elon Musk</code> or <code>Will Smith</code>) and video stylization (<i>e.g.,</i> to <code>Claymation</code> or <code>doodle art</code>).
          </p>
        <div class="container is-max-desktop">
          <div class="hero-body">
            <video poster="" id="tree" controls loop height="100%">
              <source src="./static/images/streamv2v_video.mp4"
                      type="video/mp4">
            </video>
          </div>

          <p >
            Although StreamV2V is designed for the vid2vid task, it could seamlessly integrate with the txt2img application. 
            Compared with per-image StreamDiffusion, StreamV2V continuously generates images from texts, providing a much smoother transition. 
          </p>
        <div class="container is-max-desktop">
          <div class="hero-body">
            <video poster="" id="tree" controls loop height="100%">
              <source src="./static/images/continuous_txt2img.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        <!--/ Highlight. -->

        <!-- Motivation. -->
        <h2 class="title is-3">Motivation</h2>

        <div class="subtitle has-text-justified" style="margin-bottom: 0;">
          <p>
            (a). Most of existing V2V methods process frames in batches. 
            However, batch processing necessitates loading all frames into GPU memory, thereby limiting the video length they can handle, typically up to 4 seconds.
            (b). Our StreamV2V processes frames in a streaming fashion, being able to process streaming videos in real-time.
          </p>
        </div>
        <div class="container is-max-desktop" style="display: flex; justify-content: center; margin-top: 0;">
          <div class="hero-body" style="padding-top: 10;">
            <figure>
              <img src="./static/images/batch_stream.png" alt="Embedded Image" width="640" height="360" style="display: block; margin: auto;">
            </figure>
          </div> 
        </div>
        
        <!--/ Motivation. -->

        <!-- Method. -->
        <h2 class="title is-3">Method</h2>

        <div class="subtitle has-text-justified" style="margin-bottom: 0px;">
          <p></p>
          <p>
            Overview of StreamV2V. 
            Left: StreamV2V reasons the current frame to the past by maintaining a feature bank, which stores the intermediate transformer features. 
            For new coming frames, StreamV2V fetches the stored features and uses them by Extended self-Attention (EA) and direct Feature Fusion (FF). 
            Middle: EA concatenates the stored keys <span style="font-family: monospace;">K<sub>fb</sub></span> and values <span style="font-family: monospace;">V<sub>fb</sub></span> directly to that of the current frame in the self-attention computation. 
            Right: Operating on the output of transformer blocks, FF first retrieves the similar features in the bank via a cosine similarity matrix, and then conducts a weighted sum to fuse them. 
          </p>
        </div>

        <div class="container is-max-desktop" style="padding-top: 0px;">
          <div class="hero-body" style="padding-top: 0;">
            <video poster="" id="tree" controls loop style="display: block; margin: auto; width: 100%; max-height: 100%;">
              <source src="./static/images/method.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <!--/ Method. -->

        <!-- Results. -->
        <h2 class="title is-3">Results</h2>
        <div class="subtitle has-text-justified">
          <p>
            We report the CLIP score and warp error to indicate the consistency of generated videos in the following table.
          </p>
        </div>

        <table style="width: 100%; border-collapse: collapse; margin: 1em 0;">
          <caption><strong>Quantitative metrics comparison.</strong> We bold the <strong>best</strong> result and underline the <u>second best</u>.</caption>
          <thead>
              <tr>
                  <th style="border: 1px solid #000; padding: 0.5em; text-align: center; background-color: #f2f2f2;"></th>
                  <th style="border: 1px solid #000; padding: 0.5em; text-align: center; background-color: #f2f2f2;">StreamDiffusion</th>
                  <th style="border: 1px solid #000; padding: 0.5em; text-align: center; background-color: #f2f2f2;">CoDeF</th>
                  <th style="border: 1px solid #000; padding: 0.5em; text-align: center; background-color: #f2f2f2;">Rerender</th>
                  <th style="border: 1px solid #000; padding: 0.5em; text-align: center; background-color: #f2f2f2;">TokenFlow</th>
                  <th style="border: 1px solid #000; padding: 0.5em; text-align: center; background-color: #f2f2f2;">FlowVid</th>
                  <th style="border: 1px solid #000; padding: 0.5em; text-align: center; background-color: #f2f2f2;">StreamV2V (ours)</th>
              </tr>
          </thead>
          <tbody>
              <tr>
                  <td style="border: 1px solid #000; padding: 0.5em; text-align: center;">CLIP score &#8593;</td>
                  <td style="border: 1px solid #000; padding: 0.5em; text-align: center;">95.24</td>
                  <td style="border: 1px solid #000; padding: 0.5em; text-align: center;">96.33</td>
                  <td style="border: 1px solid #000; padding: 0.5em; text-align: center;">96.20</td>
                  <td style="border: 1px solid #000; padding: 0.5em; text-align: center; font-weight: bold;">97.04</td>
                  <td style="border: 1px solid #000; padding: 0.5em; text-align: center; text-decoration: underline;">96.68</td>
                  <td style="border: 1px solid #000; padding: 0.5em; text-align: center;">96.58</td>
              </tr>
              <tr>
                  <td style="border: 1px solid #000; padding: 0.5em; text-align: center;">Warp error &#8595;</td>
                  <td style="border: 1px solid #000; padding: 0.5em; text-align: center;">117.01</td>
                  <td style="border: 1px solid #000; padding: 0.5em; text-align: center;">116.17</td>
                  <td style="border: 1px solid #000; padding: 0.5em; text-align: center; text-decoration: underline;">107.00</td>
                  <td style="border: 1px solid #000; padding: 0.5em; text-align: center;">114.25</td>
                  <td style="border: 1px solid #000; padding: 0.5em; text-align: center;">111.09</td>
                  <td style="border: 1px solid #000; padding: 0.5em; text-align: center; font-weight: bold;">102.99</td>
              </tr>
          </tbody>
      </table>
      

        <div class="subtitle has-text-justified">
          <p>
            We report our user study resultsand runtime breakdown in the following two figures. The results of different methods and user study interface could be found in <a href="https://drive.google.com/drive/folders/1YPZchMNZ25aNByd-18NUPj0iZrxe5W_-?usp=drive_link">this link</a>.
            You may also want to check 
            <a href="https://jeff-liangf.github.io/projects/streamv2v/supp/supp.html#comparisons_baselines_container">comparsion with other methods</a>,
            <a href="https://jeff-liangf.github.io/projects/streamv2v/supp/supp.html#Ablations">ablations</a>, and
            <a href="https://jeff-liangf.github.io/projects/streamv2v/supp/supp.html#Limitations">limitations</a>.
          </p>
        </div>    

        <div class="container is-max-desktop" style="display: flex; justify-content: center;">
          <div class="hero-body" style="display: flex; justify-content: space-around; width: 100%;">
            <figure style="width: 90%; text-align: left; margin: auto;">
              <img src="./static/images/user_study.png" alt="First Image" style="width: 100%; display: block; margin: auto;">
              <figcaption><strong>User study comparison.</strong> The win rate indicates the frequency our StreamV2V is preferred compared with certain counterpart.</figcaption>
            </figure>
            <figure style="width: 90%; text-align: left; margin: auto;">
              <img src="./static/images/runtime.png" alt="Second Image" style="width: 100%; display: block; margin: auto;">
              <figcaption><strong>Runtime breakdown.</strong> on one A100 GPU of generating a 4-second 512x512 resolution video with 30 FPS.</figcaption>
            </figure>
          </div>
        </div>
           
      </div>
    </div>


  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
        <pre><code>@article{liang2024looking,
  title={Looking Backward: Streaming Video-to-Video Translation with Feature Banks},
  author={Liang, Feng and Kodaira, Akio and Xu, Chenfeng and Tomizuka, Masayoshi and Keutzer, Kurt and Marculescu, Diana},
  journal={arXiv preprint arXiv:2405.15757},
  year={2024}
}
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2405.15757">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/Jeff-LiangF/streamv2v" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Thanks to <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
