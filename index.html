<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Feng(Jeff) Liang</title>

  <meta name="author" content="Feng(Jeff) Liang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/tsinghua.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Feng(Jeff) Liang</name>
              </p>
              <p>I am a researcher at <a href="https://www.sensetime.com/en/about.html">Sensetime Research</a>, where I work on efficient deep learning, computer vision and their applications. Before becoming a full-time researcher, I obtained my master degree from <a href="https://www.tsinghua.edu.cn/en/index.htm">Tsinghua University</a>, where I was advised by <a href="https://www.researchgate.net/profile/Chun_Zhang">Prof. Chun Zhang</a> and <a href="http://www.ime.tsinghua.edu.cn/publish/ime/5910/2015/20150315142048923730191/20150315142048923730191_.html">Prof. Zhihua Wang</a>. I did my bachelors at <a href="http://english.hust.edu.cn/index.htm">Huazhong University of Science and Technology</a>.
              </p>
              <p>
                My current research interests lie in machine learning, particularly in efficient/auto deep learning, as well as their applications such as neural architecture search, hardware-software co-design, computer vision, etc. I'm fortunately working with <a href="https://wlouyang.github.io/">Prof. Wanli Ouyang</a> and <a href="https://yan-junjie.github.io/">Dr. Junjie Yan</a> at SenseTime. Since June 2020, I had the privilege to serve as an intern working remotely with <a href="https://ece.duke.edu/faculty/yiran-chen">Prof. Yiran Chen</a> at <a href="https://duke.edu/">Duke University</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:liangfjeff@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/CV_JeffLiang.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=ecTFCUMAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/feng-liang-854a30150/">Linkedin</a> &nbsp/&nbsp
                <a href="https://www.zhihu.com/people/liang-feng-53">Zhihu</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/jeff.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/jeff.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
                <ul>
<!--	              <li style="line-height:30px"> <a href="https://drive.google.com/open?id=1mTBOJefYp6_ZkWwgshcikQ_AI5nw1t7_">Paper</a> on 3D sceneflow prediction accepted at CVPR 2019.</li>-->
                  <li style="line-height:30px"> <b>March 2021:</b> One paper gets accepted to CVPR 2021 as <b><font color="red">Oral</font></b>!</li>
                  <li style="line-height:30px"> <b>October 2020:</b> We release <a href="https://github.com/LaVieEnRoseSMZ/OQA">OQA code</a>. OQANets achieve a new state-of-the-art (SOTA) on quantized efficient models.</li>
                  <li style="line-height:30px"> <b>July 2020:</b> Jeff is here!</li>                </ul>
		    </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <!--<p>-->
                <!--I'm interested in computer vision, machine learning, optimization, and image processing.-->
                <!--Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images.-->
                <!--Representative papers are <span class="highlight">highlighted</span>.-->
              <!--</p>-->
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/crnas_iclr2020.png" alt="crnas" width="280" height="200" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1912.11234">
                <papertitle>Computation Reallocation for Object Detection</papertitle>
              </a>
              <br>
              <strong>Feng Liang</strong>,
              <a href="https://scholar.google.com/citations?user=rObgGWIAAAAJ&hl=en">Chen Lin</a>,
              <a href="https://scholar.google.com/citations?user=DEf4GkcAAAAJ&hl=zh-CN">Ronghao Guo</a>,
              <a href="https://msunming.github.io/">Ming Sun</a>,
              <a href="https://wuwei-ai.org/">Wei Wu</a>,
              <a href="https://yan-junjie.github.io/">Junjie Yan</a>,
              <a href="https://wlouyang.github.io/">Wanli Ouyang</a>
              <br>
              <em>ICLR</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/1912.11234">arXiv</a>,
              <a href="data/crnas_iclr2020.bib">bibtex</a>
              <p></p>
              <p>We present CRNAS that can learn computation reallocation strategies across different feature resolution and spatial position diectly on the target detection dataset.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/scalenas.png" alt="scalenas" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>ScaleNAS: One-Shot Learning of Scale-Aware Representations for Visual Recognition</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=_2cil-YAAAAJ&hl=en">Hsin-Pai Cheng*</a>,
              <strong>Feng Liang*</strong>,
              <a href="http://mengli.me/">Meng Li</a>,
              <a href="https://bowenc0221.github.io/">Bowen Cheng</a>,
              <a href="https://www.unr.edu/cse/people/feng-yan">Feng Yan</a>,
              <a href="https://ece.duke.edu/faculty/hai-helen-li">Hai Li</a>,
              <a href="https://v-chandra.github.io/about/">Vikas Chandra</a>,
              <a href="https://ece.duke.edu/faculty/yiran-chen">Yiran Chen</a>
              <br>
              * indicates equal contributions. Manuscript.
              <br>
              <a href="https://arxiv.org/abs/2011.14584">arxiv</a>,
<!--              <a href="">bibtex</a>,-->
<!--              <a href="">code</a>-->
              <p></p>
              <p>We present ScaleNAS, a one-shot learning method for exploring scale-aware representations. Scale-NAS solves multiple tasks at a time by searching multi-scalefeature  aggregation.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/oqa.png" alt="oqa" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2010.04354">
                <papertitle>Once Quantized for All: Progressively Searching for Quantized Efficient Models</papertitle>
              </a>
              <br>
              <a href="http://scholar.google.com/citations?user=o7vrw6IAAAAJ&hl=zh-CN">Mingzhu Shen*</a>,
              <strong>Feng Liang*</strong>,
              <a href="https://scholar.google.com/citations?user=ZfB7vEcAAAAJ&hl=en">Chuming Li</a>,
              <a href="https://scholar.google.com/citations?user=rObgGWIAAAAJ&hl=en">Chen Lin</a>,
              <a href="https://msunming.github.io/">Ming Sun</a>,
              <a href="https://yan-junjie.github.io/">Junjie Yan</a>,
              <a href="https://wlouyang.github.io/">Wanli Ouyang</a>
              <br>
              * indicates equal contributions. Manuscript.
              <br>
              <a href="https://arxiv.org/abs/2010.04354">arxiv</a>,
              <a href="data/oqa.bib">bibtex</a>,
              <a href="https://github.com/LaVieEnRoseSMZ/OQA">code</a>
              <p></p>
              <p>We present Once Quantized for All (OQA), a novel framework that searches for quantized efficient models and deploys their quantized weights at the same time without additional post-process.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/edo_cvpr2021.png" alt="fqn" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2012.13587">
                <papertitle>Inception Convolution with Efficient Dilation Search</papertitle>
              </a>
              <br>
              <a href="">Jie Liu</a>,
              <a href="https://scholar.google.com/citations?user=ZfB7vEcAAAAJ&hl=en">Chuming Li</a>,
              <strong>Feng Liang</strong>,
              <a href="https://scholar.google.com/citations?user=rObgGWIAAAAJ&hl=en">Chen Lin</a>,
              <a href="https://yan-junjie.github.io/">Junjie Yan</a>,
              <a href="https://wlouyang.github.io/">Wanli Ouyang</a>,
              <a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/dong-xu.html">Dong Xu</a>
              <br>
              <em>CVPR</em>, 2021, <b><font color="red">Oral</font></b>
              <br>
              <a href="https://arxiv.org/abs/2012.13587">arxiv</a>,
              <a href="data/edo_cvpr2021.bib">bibtex</a>,
              <a href="https://github.com/yifan123/IC-Conv">code</a>
              <p></p>
              <p>We proposed a new mutant of dilated convolution, namely inception (dilated) convolution where the convolutions have independent dilation among different axes, channels and layers.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/fqn_cvpr2019.png" alt="fqn" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Fully_Quantized_Network_for_Object_Detection_CVPR_2019_paper.pdf">
                <papertitle>Fully Quantized Network for Object Detection</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=2Ha91noAAAAJ&hl=en">Rundong Li</a>,
              <a href="https://scholar.google.com/citations?user=QOZnsYYAAAAJ&hl=zh-CN">Yan Wang</a>,
              <strong>Feng Liang</strong>,
              <a href="http://qinhongwei.com/academic/">Hongwei Qin</a>,
              <a href="https://yan-junjie.github.io/">Junjie Yan</a>,
              <a href="http://sist.shanghaitech.edu.cn/sist_en/2018/0820/c3846a31782/page.htm">Rui Fan</a>
              <br>
              <em>CVPR</em>, 2019
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Fully_Quantized_Network_for_Object_Detection_CVPR_2019_paper.pdf">CVF</a>,
              <a href="data/fqn_cvpr2019.bib">bibtex</a>,
              <a href="https://github.com/lirundong/quant-pack">code</a>
              <p></p>
              <p>We apply our techniques to produce fully quantized 4-bit detectors based on RetinaNet and Faster RCNN, and show that these achieve state-of-the-art performance for quantized detectors.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/nasgem_aaai2021.png" alt="nasgem" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2007.04452">
                <papertitle>NASGEM: Neural Architecture Search via Graph Embedding Method</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=_2cil-YAAAAJ&hl=en">Hsin-Pai Cheng</a>,
              <a href="http://scholar.google.com/citations?user=Lgr7--oAAAAJ&hl=en">Tunhou Zhang</a>,
              <a href="https://scholar.google.com/citations?user=UF_1P_gAAAAJ&hl=en">Yixing Zhang</a>,
              <a href="https://scholar.google.com/citations?user=h_qciFEAAAAJ&hl=zh-CN">Shiyu Li</a>,
              <strong>Feng Liang</strong>,
              <a href="https://www.unr.edu/cse/people/feng-yan">Feng Yan</a>,
              <a href="http://mengli.me/">Meng Li</a>,
              <a href="https://v-chandra.github.io/about/">Vikas Chandra</a>,
              <a href="https://ece.duke.edu/faculty/hai-helen-li">Hai Li</a>,
              <a href="https://ece.duke.edu/faculty/yiran-chen">Yiran Chen</a>
              <br>
              <em>AAAI</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2007.04452">arxiv</a>,
              <a href="data/nasgem_aaai2021.bib">bibtex</a>,
<!--              <a href="">code</a>-->
              <p></p>
              <p>We propose NASGEM which stands for Neural Architecture Search via Graph Embedding Method. NASGEM is driven by a novel graph embedding method equipped with similarity measures to capture the graph topology information.</p>
            </td>
          </tr>


        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
                <p>
                  Review papers for: IEEE TNNLS
                </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <a href="https://clustrmaps.com/site/1bhpp" title="Visit tracker">
                <img src="//www.clustrmaps.com/map_v2.png?d=6oa3ivKJIw5Vmqg_fFtgZxTmVsyrTJMJ_XKxZlDEsRI&cl=ffffff">
              </a>
              <p style="text-align:right;font-size:small;">
                Thanks to <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>